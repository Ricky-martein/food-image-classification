{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ZycMap2V2H",
        "outputId": "a47e7e95-fde1-49e3-f3ec-79c4f1aaceee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lwv-MUx2aXf"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWBQcblm2d6C"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRN67Xrv2gQz",
        "outputId": "6df6aa70-42ab-46d6-dbb0-8252c35645d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SmuKjaL2jzp",
        "outputId": "435a50ff-f0a8-41eb-e345-ef636357181e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split Summary:\n",
            "TRAIN — 2 classes, 600 images\n",
            "VAL — 2 classes, 600 images\n",
            "TEST — 2 classes, 600 images\n",
            "\n",
            " Class Matching Check:\n",
            "Classes in all splits identical?  True\n",
            "Classes only in train: set()\n",
            "Classes only in val: set()\n",
            "Classes only in test: set()\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "from collections import defaultdict\n",
        "# Defining all paths\n",
        "base_dir = '/content/drive/My Drive/Colab Notebooks/food_items'\n",
        "splits = ['train', 'val', 'test']\n",
        "split_stats = {}\n",
        "\n",
        "# Loop through each split\n",
        "for split in splits:\n",
        "    split_path = os.path.join(base_dir, split)\n",
        "    class_counts = defaultdict(int)\n",
        "    total_images = 0\n",
        "    class_names = []\n",
        "\n",
        "    for class_name in os.listdir(split_path):\n",
        "        class_path = os.path.join(split_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            image_files = [\n",
        "                f for f in os.listdir(class_path)\n",
        "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "            ]\n",
        "            count = len(image_files)\n",
        "            class_counts[class_name] = count\n",
        "            total_images += count\n",
        "            class_names.append(class_name)\n",
        "\n",
        "    split_stats[split] = {\n",
        "        'total_images': total_images,\n",
        "        'num_classes': len(class_counts),\n",
        "        'class_counts': class_counts,\n",
        "        'class_names': sorted(class_names)\n",
        "    }\n",
        "\n",
        "# Checking class consistency across all splits\n",
        "train_classes = set(split_stats['train']['class_names'])\n",
        "val_classes = set(split_stats['val']['class_names'])\n",
        "test_classes = set(split_stats['test']['class_names'])\n",
        "\n",
        "print(\"Split Summary:\")\n",
        "for split in splits:\n",
        "    print(f\"{split.upper()} — {split_stats[split]['num_classes']} classes, {split_stats[split]['total_images']} images\")\n",
        "\n",
        "print(\"\\n Class Matching Check:\")\n",
        "print(\"Classes in all splits identical? \", train_classes == val_classes == test_classes)\n",
        "print(\"Classes only in train:\", train_classes - val_classes - test_classes)\n",
        "print(\"Classes only in val:\", val_classes - train_classes - test_classes)\n",
        "print(\"Classes only in test:\", test_classes - train_classes - val_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKQv7ZKNKr2c",
        "outputId": "a323c45d-a635-44ef-9901-d8813bbdadd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 20 food classes by image count:\n",
            "main_course: 400 images\n",
            "dessert: 200 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Using the confirmed train directory\n",
        "train_dir = '/content/drive/My Drive/Colab Notebooks/food_items/train'\n",
        "\n",
        "# Counting the number of images per class\n",
        "class_counts = defaultdict(int)\n",
        "\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        class_counts[class_name] = len(image_files)\n",
        "\n",
        "# Sorting the image count (descending)\n",
        "sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "print(\"Top 20 food classes by image count:\")\n",
        "for name, count in sorted_classes[:20]:\n",
        "    print(f\"{name}: {count} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCmSExZzKzl7",
        "outputId": "8611b61b-f31c-4182-b208-41a36c8e6b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgDhNHrgK00E",
        "outputId": "b8e0445e-0316-4eaa-f6d5-d15f2de914d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': {'main_course': 400, 'dessert': 200},\n",
              " 'val': {'main_course': 400, 'dessert': 200},\n",
              " 'test': {'dessert': 200, 'main_course': 400}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define source and target directories\n",
        "original_base = Path(\"/content/drive/MyDrive/food_subset\")\n",
        "target_dir = Path(\"/content/drive/MyDrive/food_subset_shared_final\")\n",
        "\n",
        "# Step 3: Define selected categories\n",
        "categories = {\n",
        "    \"main_course\": [\"chicken_curry\", \"hamburger\", \"fried_rice\", \"caesar_salad\"],\n",
        "    \"dessert\": [\"apple_pie\", \"ice_cream\", \"carrot_cake\", \"baklava\"]\n",
        "}\n",
        "\n",
        "# Step 4: Limit number of images per class\n",
        "MAX_IMAGES = 100\n",
        "\n",
        "# Step 5: Copy files to new structure\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    src_split_path = original_base / split\n",
        "    dst_split_path = target_dir / split\n",
        "\n",
        "    for label, foods in categories.items():\n",
        "        for food in foods:\n",
        "            src = src_split_path / food\n",
        "            dst = dst_split_path / label\n",
        "            dst.mkdir(parents=True, exist_ok=True)\n",
        "            if src.exists():\n",
        "                files = sorted(os.listdir(src))[:MAX_IMAGES]\n",
        "                for file in files:\n",
        "                    full_file = src / file\n",
        "                    if full_file.is_file():\n",
        "                        shutil.copy(full_file, dst)\n",
        "\n",
        "# Step 6: summary of what was saved\n",
        "subset_summary = {\n",
        "    split: {\n",
        "        label: len(os.listdir(target_dir / split / label))\n",
        "        for label in os.listdir(target_dir / split)\n",
        "    }\n",
        "    for split in [\"train\", \"val\", \"test\"]\n",
        "}\n",
        "\n",
        "subset_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnOXqiOOK54D",
        "outputId": "2fb1ecce-3a9d-47fe-e1d9-b3a0685086d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN → 600 images (33.33%)\n",
            "VAL   → 600 images (33.33%)\n",
            "TEST  → 600 images (33.33%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/food_subset_shared_final\"\n",
        "split_counts = {}\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    total = 0\n",
        "    split_path = os.path.join(base_dir, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        print(f\"Folder not found: {split_path}\")\n",
        "        continue\n",
        "    for category in os.listdir(split_path):\n",
        "        class_dir = os.path.join(split_path, category)\n",
        "        if os.path.isdir(class_dir):\n",
        "            total += len(os.listdir(class_dir))\n",
        "    split_counts[split] = total\n",
        "\n",
        "total_images = sum(split_counts.values())\n",
        "\n",
        "if total_images == 0:\n",
        "    print(\" No images found in any split. Please check the folder structure.\")\n",
        "else:\n",
        "    for split, count in split_counts.items():\n",
        "        ratio = round((count / total_images) * 100, 2)\n",
        "        print(f\"{split.upper():<5} → {count} images ({ratio}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXr5zgYMLARW",
        "outputId": "dc97be4f-159a-48a0-ea3d-af584aaf3982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN → 600 images (33.33%)\n",
            "VAL   → 600 images (33.33%)\n",
            "TEST  → 600 images (33.33%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/food_subset_shared_final\"\n",
        "split_counts = {}\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    total = 0\n",
        "    split_path = os.path.join(base_dir, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        print(f\"Folder not found: {split_path}\")\n",
        "        continue\n",
        "    for category in os.listdir(split_path):\n",
        "        class_dir = os.path.join(split_path, category)\n",
        "        if os.path.isdir(class_dir):\n",
        "            total += len(os.listdir(class_dir))\n",
        "    split_counts[split] = total\n",
        "\n",
        "total_images = sum(split_counts.values())\n",
        "\n",
        "if total_images == 0:\n",
        "    print(\" No images found in any split. Please check the folder structure.\")\n",
        "else:\n",
        "    for split, count in split_counts.items():\n",
        "        ratio = round((count / total_images) * 100, 2)\n",
        "        print(f\"{split.upper():<5} → {count} images ({ratio}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ7i94_mLEff",
        "outputId": "406b863c-6062-40f6-f4d5-ec4c624c1abe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing categories: 100%|██████████| 2/2 [00:24<00:00, 12.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Done: Images re-split into 70% train, 10% val, 10% test.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 1: Set paths\n",
        "original_data_dir = \"/content/drive/MyDrive/food_subset_shared_final/train\"\n",
        "resplit_dir = \"/content/drive/MyDrive/food_subset_resplit_70_10_10\"\n",
        "\n",
        "\n",
        "# Step 3: Clean old folder if exists\n",
        "if os.path.exists(resplit_dir):\n",
        "    shutil.rmtree(resplit_dir)\n",
        "\n",
        "# Step 4: Create folder structure\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "categories = [\"main_course\", \"dessert\"]\n",
        "\n",
        "for split in splits:\n",
        "    for category in categories:\n",
        "        os.makedirs(os.path.join(resplit_dir, split, category), exist_ok=True)\n",
        "\n",
        "# Step 5: Perform splitting\n",
        "for category in tqdm(categories, desc=\"Processing categories\"):\n",
        "    category_path = os.path.join(original_data_dir, category)\n",
        "    files = [f for f in os.listdir(category_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    random.shuffle(files)\n",
        "\n",
        "    total = len(files)\n",
        "    train_end = int(0.7 * total)\n",
        "    val_end = train_end + int(0.1 * total)\n",
        "\n",
        "    split_map = {\n",
        "        \"train\": files[:train_end],\n",
        "        \"val\": files[train_end:val_end],\n",
        "        \"test\": files[val_end:]\n",
        "    }\n",
        "\n",
        "    for split, split_files in split_map.items():\n",
        "        for f in split_files:\n",
        "            src = os.path.join(category_path, f)\n",
        "            dst = os.path.join(resplit_dir, split, category, f)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "print(\" Done: Images re-split into 70% train, 10% val, 10% test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc_NpSK4LJTO",
        "outputId": "da2d010b-b770-4b06-971a-ad0e89621b3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Image Split Summary:\n",
            "TRAIN → 420 images (70.0%)\n",
            "VAL   → 60 images (10.0%)\n",
            "TEST  → 120 images (20.0%)\n"
          ]
        }
      ],
      "source": [
        "resplit_dir = \"/content/drive/MyDrive/food_subset_resplit_70_10_10\"\n",
        "\n",
        "split_counts = {}\n",
        "total_images = 0\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    count = 0\n",
        "    split_path = os.path.join(resplit_dir, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        print(f\"Folder not found: {split_path}\")\n",
        "        continue\n",
        "    for category in os.listdir(split_path):\n",
        "        category_path = os.path.join(split_path, category)\n",
        "        count += len([img for img in os.listdir(category_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "    split_counts[split] = count\n",
        "    total_images += count\n",
        "\n",
        "#summary\n",
        "print(\"Final Image Split Summary:\")\n",
        "for split, count in split_counts.items():\n",
        "    ratio = round((count / total_images) * 100, 2) if total_images else 0\n",
        "    print(f\"{split.upper():<5} → {count} images ({ratio}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNtQmhRQLU46",
        "outputId": "d1bf7433-eb63-4629-b452-3e1ded9f29bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:16<00:00, 21.4MiB/s]\n",
            "Filtering main_course: 100%|██████████| 280/280 [05:22<00:00,  1.15s/it]\n",
            "Filtering dessert: 100%|██████████| 140/140 [03:05<00:00,  1.33s/it]\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install CLIP dependencies\n",
        "!pip install -q ftfy regex tqdm\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# STEP 3: Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# STEP 4: Set paths\n",
        "base_path = \"/content/drive/MyDrive/food_subset_resplit_70_10_10/train\"\n",
        "output_path = \"/content/drive/MyDrive/filtered_food_subset/train\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# STEP 5: Define valid \"pure food\" prompts\n",
        "keep_prompts = [\n",
        "    \"a plate of food\",\n",
        "    \"a cooked dish\",\n",
        "    \"delicious meal\",\n",
        "    \"plated meal\",\n",
        "    \"served food\"\n",
        "]\n",
        "text_tokens = clip.tokenize(keep_prompts).to(device)\n",
        "\n",
        "# STEP 6: Filter images based on similarity to food prompts\n",
        "for category in os.listdir(base_path):\n",
        "    cat_path = os.path.join(base_path, category)\n",
        "    if not os.path.isdir(cat_path):\n",
        "        continue\n",
        "\n",
        "    save_path = os.path.join(output_path, category)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    for filename in tqdm(os.listdir(cat_path), desc=f\"Filtering {category}\"):\n",
        "        img_path = os.path.join(cat_path, filename)\n",
        "        try:\n",
        "            image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                image_features = model.encode_image(image)\n",
        "                text_features = model.encode_text(text_tokens)\n",
        "\n",
        "                similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
        "                best_score = similarity[0].max().item()\n",
        "\n",
        "                # Keep image only if it strongly matches \"pure food\"\n",
        "                if best_score > 0.25:\n",
        "                    Image.open(img_path).save(os.path.join(save_path, filename))\n",
        "        except:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHHWwOALZNx",
        "outputId": "80cac866-d650-4410-8aa3-93f44ca0c836"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filtering val/dessert: 100%|██████████| 20/20 [00:42<00:00,  2.11s/it]\n",
            "Filtering val/main_course: 100%|██████████| 40/40 [01:10<00:00,  1.75s/it]\n",
            "Filtering test/dessert: 100%|██████████| 40/40 [01:03<00:00,  1.59s/it]\n",
            "Filtering test/main_course: 100%|██████████| 80/80 [02:37<00:00,  1.96s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Define prompts to keep food-only images\n",
        "keep_prompts = [\n",
        "    \"a plate of food\", \"a cooked dish\", \"delicious meal\",\n",
        "    \"plated meal\", \"served food\"\n",
        "]\n",
        "text_tokens = clip.tokenize(keep_prompts).to(device)\n",
        "\n",
        "# Base directories\n",
        "base_input_dir = \"/content/drive/MyDrive/food_subset_resplit_70_10_10\"\n",
        "base_output_dir = \"/content/drive/MyDrive/filtered_food_subset\"\n",
        "os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "# Only filter val and test\n",
        "for split in [\"val\", \"test\"]:\n",
        "    input_dir = os.path.join(base_input_dir, split)\n",
        "    output_dir = os.path.join(base_output_dir, split)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for category in os.listdir(input_dir):\n",
        "        cat_path = os.path.join(input_dir, category)\n",
        "        if not os.path.isdir(cat_path):\n",
        "            continue\n",
        "\n",
        "        save_path = os.path.join(output_dir, category)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        for filename in tqdm(os.listdir(cat_path), desc=f\"Filtering {split}/{category}\"):\n",
        "            img_path = os.path.join(cat_path, filename)\n",
        "            try:\n",
        "                image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    image_features = model.encode_image(image)\n",
        "                    text_features = model.encode_text(text_tokens)\n",
        "                    similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
        "                    best_score = similarity[0].max().item()\n",
        "\n",
        "                    if best_score > 0.25:\n",
        "                        Image.open(img_path).save(os.path.join(save_path, filename))\n",
        "            except:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0aqrwcULeZw",
        "outputId": "36bd30e9-a335-467e-f243-4218a3f03e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Total images across all splits: 1393\n",
            "\n",
            "TRAIN → 600 images (43.07%)\n",
            "VAL   → 321 images (23.04%)\n",
            "TEST  → 472 images (33.88%)\n"
          ]
        }
      ],
      "source": [
        "filtered_base = \"/content/drive/MyDrive/filtered_food_subset\"\n",
        "\n",
        "def check_split_ratios(base_dir):\n",
        "    split_counts = {}\n",
        "    total_images = 0\n",
        "\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        split_path = os.path.join(base_dir, split)\n",
        "        image_count = 0\n",
        "\n",
        "        if not os.path.exists(split_path):\n",
        "            print(f\" Missing folder: {split_path}\")\n",
        "            continue\n",
        "\n",
        "        for category in os.listdir(split_path):\n",
        "            category_path = os.path.join(split_path, category)\n",
        "            if os.path.isdir(category_path):\n",
        "                count = len([\n",
        "                    f for f in os.listdir(category_path)\n",
        "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "                ])\n",
        "                image_count += count\n",
        "\n",
        "        split_counts[split] = image_count\n",
        "        total_images += image_count\n",
        "\n",
        "    print(f\"\\n Total images across all splits: {total_images}\\n\")\n",
        "    for split, count in split_counts.items():\n",
        "        percent = (count / total_images * 100) if total_images > 0 else 0\n",
        "        print(f\"{split.upper():<5} → {count} images ({percent:.2f}%)\")\n",
        "\n",
        "# Run the check\n",
        "check_split_ratios(filtered_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEJYcQSS5ULH",
        "outputId": "648796f5-a7e2-4b08-9b78-354248e0f067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 600 images belonging to 2 classes.\n",
            "Found 321 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5151 - loss: 0.9211"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 8s/step - accuracy: 0.5180 - loss: 0.9168 - val_accuracy: 0.8660 - val_loss: 0.3906 - learning_rate: 1.0000e-04\n",
            "Epoch 2/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step - accuracy: 0.8103 - loss: 0.4006 - val_accuracy: 0.8972 - val_loss: 0.2529 - learning_rate: 1.0000e-04\n",
            "Epoch 3/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - accuracy: 0.9010 - loss: 0.2220 - val_accuracy: 0.9128 - val_loss: 0.2034 - learning_rate: 1.0000e-04\n",
            "Epoch 4/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 4s/step - accuracy: 0.8920 - loss: 0.2309 - val_accuracy: 0.9221 - val_loss: 0.1683 - learning_rate: 1.0000e-04\n",
            "Epoch 5/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 4s/step - accuracy: 0.9261 - loss: 0.1686 - val_accuracy: 0.9315 - val_loss: 0.1453 - learning_rate: 1.0000e-04\n",
            "Epoch 6/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 4s/step - accuracy: 0.9376 - loss: 0.1355 - val_accuracy: 0.9408 - val_loss: 0.1248 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 4s/step - accuracy: 0.9553 - loss: 0.1559 - val_accuracy: 0.9626 - val_loss: 0.1045 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 0.9627 - loss: 0.0924 - val_accuracy: 0.9688 - val_loss: 0.0915 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 4s/step - accuracy: 0.9800 - loss: 0.0760 - val_accuracy: 0.9844 - val_loss: 0.0710 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.9659 - loss: 0.0945 - val_accuracy: 0.9751 - val_loss: 0.0602 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 4s/step - accuracy: 0.9835 - loss: 0.0666 - val_accuracy: 0.9813 - val_loss: 0.0591 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - accuracy: 0.9703 - loss: 0.0619 - val_accuracy: 0.9626 - val_loss: 0.0854 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.9695 - loss: 0.0736 - val_accuracy: 0.9751 - val_loss: 0.0785 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.9881 - loss: 0.0497 - val_accuracy: 0.9751 - val_loss: 0.0607 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 0.9880 - loss: 0.0507 - val_accuracy: 0.9720 - val_loss: 0.0613 - learning_rate: 2.0000e-05\n",
            "Epoch 16/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 4s/step - accuracy: 0.9787 - loss: 0.0461 - val_accuracy: 0.9720 - val_loss: 0.0585 - learning_rate: 2.0000e-05\n",
            "Epoch 17/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 4s/step - accuracy: 0.9826 - loss: 0.0549 - val_accuracy: 0.9751 - val_loss: 0.0568 - learning_rate: 2.0000e-05\n",
            "Epoch 18/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.9829 - loss: 0.0492 - val_accuracy: 0.9813 - val_loss: 0.0527 - learning_rate: 2.0000e-05\n",
            "Epoch 19/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0185 - val_accuracy: 0.9782 - val_loss: 0.0515 - learning_rate: 2.0000e-05\n",
            "Epoch 20/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 4s/step - accuracy: 0.9977 - loss: 0.0263 - val_accuracy: 0.9782 - val_loss: 0.0511 - learning_rate: 2.0000e-05\n",
            "Epoch 21/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.9906 - loss: 0.0297 - val_accuracy: 0.9813 - val_loss: 0.0464 - learning_rate: 2.0000e-05\n",
            "Epoch 22/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.9936 - loss: 0.0313 - val_accuracy: 0.9844 - val_loss: 0.0423 - learning_rate: 2.0000e-05\n",
            "Epoch 23/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.9929 - loss: 0.0239 - val_accuracy: 0.9844 - val_loss: 0.0370 - learning_rate: 2.0000e-05\n",
            "Epoch 24/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 0.9829 - loss: 0.0463 - val_accuracy: 0.9875 - val_loss: 0.0308 - learning_rate: 2.0000e-05\n",
            "Epoch 25/25\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.9908 - loss: 0.0297 - val_accuracy: 0.9875 - val_loss: 0.0272 - learning_rate: 2.0000e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "train_dir = '/content/drive/MyDrive/filtered_food_subset/train'\n",
        "val_dir = '/content/drive/MyDrive/filtered_food_subset/val'\n",
        "\n",
        "\n",
        "# Check if directories exist and contain files\n",
        "if not os.path.exists(train_dir) or not any(os.scandir(train_dir)):\n",
        "    raise FileNotFoundError(f\"Training directory not found or is empty: {train_dir}. Please ensure the filtering process successfully saved images to this location.\")\n",
        "\n",
        "if not os.path.exists(val_dir) or not any(os.scandir(val_dir)):\n",
        "     raise FileNotFoundError(f\"Validation directory not found or is empty: {val_dir}. Please ensure the filtering process successfully saved images to this location.\")\n",
        "\n",
        "\n",
        "# Improved data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Calculate class weights\n",
        "# Use train_gen.classes which is populated after flow_from_directory\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_gen.classes),\n",
        "    y=train_gen.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "\n",
        "# Load MobileNetV2 base model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Fine-tune last 30 layers instead of 10\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(64, activation='relu')(x)  # Added layer for deeper learning\n",
        "output = Dense(2, activation='softmax')(x)  # Adjust if more than 2 classes\n",
        "\n",
        "# Final model\n",
        "mobilenet_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "mobilenet_model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "# Train model with class weights\n",
        "mobilenet_model.fit(train_gen, validation_data=val_gen, epochs=25, callbacks=callbacks, class_weight=class_weights_dict)\n",
        "\n",
        "# Save model\n",
        "mobilenet_model.save('/content/drive/MyDrive/filtered_food_subset/mobilenetv2_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "21cce027",
        "outputId": "3da7ad46-0c45-4ff2-8bca-85540fc0ef20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution in the training set:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "defaultdict(int, {'main_course': 400, 'dessert': 200})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Class distribution in the training set:\")\n",
        "display(split_stats['train']['class_counts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eegIaExU4Azn",
        "outputId": "48946731-f959-452b-8817-e142c2cd5d9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 600 images belonging to 2 classes.\n",
            "Class names in order: ['dessert', 'main_course']\n",
            "Input image shape: (512, 384, 3)\n",
            "Resized image shape: (224, 224, 3)\n",
            "Image shape with batch dimension: (1, 224, 224, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Raw prediction output: [0.23219042 0.76780957]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'dessert': 0.23219041526317596, 'main_course': 0.7678095698356628}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the trained MobileNetV2 model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/filtered_food_subset/mobilenetv2_model.h5')\n",
        "\n",
        "# Define class_names using the class_indices from the training generator\n",
        "# This ensures the order matches the model's output\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # Create a simple generator to get class_indices\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/filtered_food_subset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "\n",
        "print(f\"Class names in order: {class_names}\")\n",
        "\n",
        "\n",
        "def predict_image_debug(image_path):\n",
        "  # Load the image from the provided path\n",
        "  img = Image.open(image_path)\n",
        "  img_array = np.array(img)\n",
        "  print(f\"Input image shape: {img_array.shape}\")\n",
        "\n",
        "  # Resize the image to the target size (224, 224) - Model was trained on 224x224\n",
        "  img_resized = tf.image.resize(img_array, (224, 224))\n",
        "  print(f\"Resized image shape: {img_resized.shape}\")\n",
        "\n",
        "  img_4d = np.expand_dims(img_resized, axis=0)  # Add batch dimension\n",
        "  print(f\"Image shape with batch dimension: {img_4d.shape}\")\n",
        "\n",
        "  prediction = model.predict(img_4d)[0]\n",
        "  print(f\"Raw prediction output: {prediction}\")\n",
        "\n",
        "  return {class_names[i]: float(prediction[i]) for i in range(len(class_names))}\n",
        "\n",
        "# Replace 'path/to/your/dessert_image.jpg' with the actual path to your image\n",
        "predict_image_debug('/content/drive/MyDrive/filtered_food_subset/test/dessert/104465.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "TQZHmayu4edf",
        "outputId": "7745d851-0da4-4357-f1e5-2eab4f3a88f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 600 images belonging to 2 classes.\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4c6abf5a10de3edb6f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4c6abf5a10de3edb6f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# Assuming the model and class_names are already loaded and defined\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/filtered_food_subset/mobilenetv2_model.h5')\n",
        "# class_names = ['dessert', 'main_course'] # Ensure this matches the model's output order\n",
        "\n",
        "def classify_image(image):\n",
        "  # The input 'image' is a PIL Image object from Gradio\n",
        "  img_array = np.array(image)\n",
        "  # Ensure image is in RGB format if it's grayscale or has an alpha channel\n",
        "  if img_array.shape[-1] == 4:\n",
        "      img_array = img_array[:, :, :3]\n",
        "  elif len(img_array.shape) == 2:\n",
        "      img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "\n",
        "  img_resized = tf.image.resize(img_array, (224, 224))\n",
        "  img_4d = np.expand_dims(img_resized, axis=0)  # Add batch dimension\n",
        "  img_4d = img_4d / 255.0 # Rescale the image\n",
        "\n",
        "  prediction = model.predict(img_4d)[0]\n",
        "\n",
        "  return {class_names[i]: float(prediction[i]) for i in range(len(class_names))}\n",
        "\n",
        "# Define class_names using the class_indices from the training generator to ensure correct order\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # Create a simple generator to get class_indices\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/filtered_food_subset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "\n",
        "\n",
        "image_input = gr.Image(label=\"Upload Food Image\")\n",
        "label_output = gr.Label(num_top_classes=2)\n",
        "\n",
        "gr.Interface(fn=classify_image, inputs=image_input, outputs=label_output).launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4ee6256"
      },
      "outputs": [],
      "source": [
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #f0f0f0; /* Change this to your desired background color */\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "586745a9"
      },
      "source": [
        "Now, update your `gr.Interface` call to include the `css` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10e5e0eb"
      },
      "outputs": [],
      "source": [
        "# Brainstorming typical restaurant menu elements and mapping to Gradio components:\n",
        "# Menu Sections: Appetizers, Main Courses, Desserts, Drinks -> Can be represented by gr.Tabs or separate sections within gr.Blocks using rows/columns.\n",
        "# Dish Name: -> gr.Label or gr.Textbox (display only)\n",
        "# Dish Description: -> gr.Textbox (display only)\n",
        "# Dish Image: -> gr.Image\n",
        "# Dish Price: -> gr.Label or gr.Number (display only)\n",
        "\n",
        "# Considering using gr.Blocks for flexible layout:\n",
        "# Use gr.Blocks to create the overall page structure.\n",
        "# Use gr.Tabs for different menu sections (Appetizers, Main Courses, etc.).\n",
        "# Within each tab (or section), use gr.Column or gr.Row to arrange individual menu items.\n",
        "# For each menu item, use a combination of gr.Image, gr.Label, and gr.Textbox to display the image, name, description, and price.\n",
        "\n",
        "# Outline of the Gradio interface structure:\n",
        "# gr.Blocks:\n",
        "#   gr.HTML (Optional: for a title/header)\n",
        "#   gr.Tabs:\n",
        "#     gr.Tab (label=\"Appetizers\"):\n",
        "#       gr.Row: # For the first appetizer\n",
        "#         gr.Image\n",
        "#         gr.Column:\n",
        "#           gr.Label (Dish Name)\n",
        "#           gr.Textbox (Dish Description)\n",
        "#           gr.Label (Dish Price)\n",
        "#       gr.Row: # For the second appetizer\n",
        "#         gr.Image\n",
        "#         gr.Column:\n",
        "#           gr.Label (Dish Name)\n",
        "#           gr.Textbox (Dish Description)\n",
        "#           gr.Label (Dish Price)\n",
        "#       # Add more Rows for other appetizers\n",
        "#     gr.Tab (label=\"Main Courses\"):\n",
        "#       # Similar structure as Appetizers tab\n",
        "#     gr.Tab (label=\"Desserts\"):\n",
        "#       # Similar structure as Appetizers tab\n",
        "#     gr.Tab (label=\"Drinks\"):\n",
        "#       # Similar structure as Appetizers tab\n",
        "#   gr.HTML (Optional: for a footer)\n",
        "\n",
        "print(\"Gradio interface structure outlined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8636009f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the trained model (assuming this is already loaded in your environment, but including for clarity)\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/filtered_food_subset/mobilenetv2_model.h5')\n",
        "\n",
        "# Define class_names using the class_indices from the training generator\n",
        "# This ensures the order matches the model's output\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # Create a simple generator to get class_indices\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/filtered_food_subset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "\n",
        "print(f\"Class names in order: {class_names}\")\n",
        "\n",
        "\n",
        "def predict_image_debug(image_path):\n",
        "  # Load the image from the provided path\n",
        "  img = Image.open(image_path)\n",
        "  img_array = np.array(img)\n",
        "  print(f\"Input image shape: {img_array.shape}\")\n",
        "\n",
        "  # Resize the image to the target size (224, 224) - Model was trained on 224x224\n",
        "  img_resized = tf.image.resize(img_array, (224, 224))\n",
        "  print(f\"Resized image shape: {img_resized.shape}\")\n",
        "\n",
        "  img_4d = np.expand_dims(img_resized, axis=0)  # Add batch dimension\n",
        "  print(f\"Image shape with batch dimension: {img_4d.shape}\")\n",
        "\n",
        "  prediction = model.predict(img_4d)[0]\n",
        "  print(f\"Raw prediction output: {prediction}\")\n",
        "\n",
        "  return {class_names[i]: float(prediction[i]) for i in range(len(class_names))}\n",
        "\n",
        "# Replace 'path/to/your/dessert_image.jpg' with the actual path to your image\n",
        "predict_image_debug('/content/drive/MyDrive/filtered_food_subset/test/dessert/104465.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02ecab30"
      },
      "outputs": [],
      "source": [
        "print(\"Class indices from the training generator:\")\n",
        "display(train_gen.class_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "afa942eb",
        "outputId": "43755ca9-e544-4106-cb52-271bb51d5c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 600 images belonging to 2 classes.\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b03f74abe1efb75000.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b03f74abe1efb75000.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-23-2185140615.py\", line 23, in classify_image\n",
            "    prediction = model.predict(img_4d)[0]\n",
            "                 ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'CLIP' object has no attribute 'predict'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b03f74abe1efb75000.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# Assuming the model and class_names are already loaded and defined\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/filtered_food_subset/mobilenetv2_model.h5')\n",
        "# class_names = ['dessert', 'main_course'] # Ensure this matches the model's output order\n",
        "\n",
        "def classify_image(image):\n",
        "  # The input 'image' is a PIL Image object from Gradio\n",
        "  img_array = np.array(image)\n",
        "  # Ensure image is in RGB format if it's grayscale or has an alpha channel\n",
        "  if img_array.shape[-1] == 4:\n",
        "      img_array = img_array[:, :, :3]\n",
        "  elif len(img_array.shape) == 2:\n",
        "      img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "\n",
        "  img_resized = tf.image.resize(img_array, (224, 224))\n",
        "  img_4d = np.expand_dims(img_resized, axis=0)  # Add batch dimension\n",
        "  img_4d = img_4d / 255.0 # Rescale the image\n",
        "\n",
        "  prediction = model.predict(img_4d)[0]\n",
        "\n",
        "  return {class_names[i]: float(prediction[i]) for i in range(len(class_names))}\n",
        "\n",
        "# Define class_names using the class_indices from the training generator to ensure correct order\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # Create a simple generator to get class_indices\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/filtered_food_subset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "class_names = list(train_gen.class_indices.keys())\n",
        "\n",
        "\n",
        "image_input = gr.Image(label=\"Upload Food Image\")\n",
        "label_output = gr.Label(num_top_classes=2)\n",
        "\n",
        "gr.Interface(fn=classify_image, inputs=image_input, outputs=label_output).launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}